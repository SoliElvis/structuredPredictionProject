\documentclass{article}

\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amsbsy,amssymb,amsfonts,amsthm}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{color}
\usepackage{xspace} % Correct macro spacing
\usepackage{times}
\usepackage{graphicx,subfigure}
%\usepackage[small,bf]{caption}
\usepackage{algorithm,algorithmic} 
\usepackage{hyperref}
\usepackage[backend=bibtex,style=authoryear-ibid]{biblatex}
\addbibresource{proposal.bib}

\usepackage{xcolor}
\usepackage{shadethm}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{}
\rhead{\thepage}


\newshadetheorem{thm}{Theorem}
\newshadetheorem{defn}[thm]{Definition}
\newshadetheorem{assm}[thm]{Assumption}
\newshadetheorem{prop}[thm]{Property}
\newshadetheorem{eg}[thm]{Example}


\definecolor{shadethmcolor}{HTML}{F0F0F0}
%\definecolor{shadethmcolor}{HTML}{EDEDED}
%\definecolor{shadethmcolor}{HTML}{EDF8FF}
%\definecolor{shaderulecolor}{HTML}{EDF8FF}
%\definecolor{shaderulecolor}{HTML}{45CFFF}
\setlength{\shadeboxrule}{.4pt}


\setlength\parindent{0pt}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\title{Scalable Optimization algorithms\\ \& \\ Modern Approaches to Structured
Prediction}
\date{}

\begin{document} 

\maketitle


\vspace{-0.5in}
\begin{center}
William St-Arnaud, Elyes Lamouchi, Fr\'ed\'eric Boileau
\end{center}
\vspace{0.2in}


\section*{Introduction}

Structured prediction is concerned with predicting labels from a given
set of features where the labels have some inherent structure which should
be considered when training the model. The structures considered are usually
combinatorial in nature which means comparing the scores of two assignments
can be a challenge with respect to tractability. We review some of the ways
this is tackled with a large margin approach in the modern literature.
We thus focus on the structural SVM framework, which has computational
advantage over MLE approaches by dispensing with the need to find the
partition function which is \#P-complete in general; e.g. for
matchings and mincuts.(\cite{dualextraSimon}).\\

Using convex analysis, it is not hard to see that since the primal optimization
problem of struct SVM has an exponential number of constraints in the length of
the input, the dual will have an exponential number of variables. One approach is
thus to solve the dual while imposing some notion of sparsity on the dual
variables. Intuitively, the success of this approach can be explained by the fact
that we only expect a small number of constraints to be active or nearly active
at a certain point. \\




The Frank-Wolfe algorithm (\cite{f-w}) is a popular method for constrained
convex optimization, consisting in taking a first-order approximation of the
objective function and doing a linear search over the set of constraints to
update the current point, leveraging sparsity . Over the years, many
improvements and alternatives were developed to address the particularities of
various frameworks. For example Jaggi \cite{Jaggi:229246} presents a variety of
stronger convergence results and duality gaps for a number of variants of the
Frank-Wolfe algorithm
(e.g. F-W with approximate linear subproblems, away steps, etc.).\\

A primary limitation of Frank-Wolfe is that it requires a pass through the
entire dataset per iteration, which makes the stochastic subgradient method more
practical. Adressing this problem, Lacoste-Julien et al.
\cite{DBLP:journals/corr/abs-1207-4747} propose a randomized block-coordinate
variant of the F-W algorithm inspired from coordinate descent methods. The paper
goes on to demonstrate its use in the case of the structured SVM and the
advantages it yields over its competitors.\\ More recently, S. Lacoste-Julien
and M.Jaggi \cite{2015arXiv151105932L} showed that the "Away-steps", "Pairwise
FW", "fully-corrective FW", and "Minimum norm point" variants of the F-W
algorithm all achieve linear convergence under a weaker condition than strong
convexity.

Another approach developed in (\cite{dualextraSimon}) is to to leverage the
min-max structure of the loss-augmented decoding problem. The latter arises
naturally in struct SVM as we are minimizing the loss over the labels which
maximize the score. In many cases of interest, the saddle-point problem has a
linear objective function allowing us to use efficient convex relaxation
techniques. Moreover, we can exploit the structure of the problem in conjuction
with ideas from combinatorial optimization to guarantee tractability in some
well known cases.

\section*{Project}
The type of project we wish to present is at the crossing of two lines of work:
analysis and practical evaluation. Our project will, therefore, be composed of
two main parts:
\begin{itemize}
    \item The first part is a literature review on the subject. We intend to go
deeper into some of the papers cited above, studying the assumptions they make
and identifying the critical points in the proofs that forbid using softer
assumptions. We will also compare the papers, showing how they relate to each
other and how they overcome previous difficulties related to structured
predictions. Finally, this part would serve as an overview of all previous
results that will allow us to identify the insights and limitations of this
line of work and potentially some gaps which could be used
as a starting point for future research.
    \item The second part consists of a practical evaluation of the method
proposed in and of other methods depending on the time constraints. The idea is
to see first-handedly the effect of the various variants of FW on real data.
This way, we can better compare/justify the usage of these methods in real-world
scenarios. We intend to run some evaluation of the model on both toy/synthetic
datasets and real datasets with a few numbers of network configurations.
\end{itemize}


Our project will take the form of an article detailing the analysis of the
papers and our experiments on real data, as well as a repository including the
code used for the experiments.

\nocite{*}
\printbibliography
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
