
Cornell University
We gratefully acknowledge support from
the Simons Foundation and member institutions.
arXiv.org > cs > arXiv:1708.05978
( Help | Advanced search )
Full-text links:
Download:

    PDF
    Other formats

( license )
Current browse context:
cs.LG
< prev  |  next >
new  | recent  | 1708
Change to browse by:
cs
math
math.OC
stat
stat.ML
References & Citations

    NASA ADS

DBLP - CS Bibliography
listing | bibtex
Tianyi Lin
Linbo Qiao
Teng Zhang
Jiashi Feng
Bofeng Zhang
Google Scholar
Bookmark
( what is this? )
CiteULike logo BibSonomy logo Mendeley logo Reddit logo ScienceWISE logo
Computer Science > Machine Learning
Title: Stochastic Primal-Dual Proximal ExtraGradient Descent for Compositely Regularized Optimization
Authors: Tianyi Lin , Linbo Qiao , Teng Zhang , Jiashi Feng , Bofeng Zhang
(Submitted on 20 Aug 2017 ( v1 ), last revised 1 Feb 2018 (this version, v4))

    Abstract: We consider a wide range of regularized stochastic minimization problems with two regularization terms, one of which is composed with a linear function. This optimization model abstracts a number of important applications in artificial intelligence and machine learning, such as fused Lasso, fused logistic regression, and a class of graph-guided regularized minimization. The computational challenges of this model are in two folds. On one hand, the closed-form solution of the proximal mapping associated with the composed regularization term or the expected objective function is not available. On the other hand, the calculation of the full gradient of the expectation in the objective is very expensive when the number of input data samples is considerably large. To address these issues, we propose a stochastic variant of extra-gradient type methods, namely \textsf{Stochastic Primal-Dual Proximal ExtraGradient descent (SPDPEG)}, and analyze its convergence property for both convex and strongly convex objectives. For general convex objectives, the uniformly average iterates generated by \textsf{SPDPEG} converge in expectation with O ( 1 / t √ ) rate. While for strongly convex objectives, the uniformly and non-uniformly average iterates generated by \textsf{SPDPEG} converge with O ( log ( t ) / t ) and O ( 1 / t ) rates, respectively. The order of the rate of the proposed algorithm is known to match the best convergence rate for first-order stochastic algorithms. Experiments on fused logistic regression and graph-guided regularized logistic regression problems show that the proposed algorithm performs very efficiently and consistently outperforms other competing algorithms. 

Subjects: 	Machine Learning (cs.LG) ; Optimization and Control (math.OC); Machine Learning (stat.ML)
DOI : 	10.1016/j.neucom.2017.07.066
Cite as: 	arXiv:1708.05978 [cs.LG]
  	(or arXiv:1708.05978v4 [cs.LG] for this version)
Try the Bibliographic Explorer
(can be disabled at any time)
Enable Don't show again
Bibliographic data
[ Enable Bibex ( What is Bibex? )]
Submission history
From: Linbo Qiao [ view email ]
[v1] Sun, 20 Aug 2017 14:53:12 UTC (501 KB)
[v2] Sun, 27 Aug 2017 17:37:03 UTC (509 KB)
[v3] Tue, 12 Dec 2017 13:48:30 UTC (509 KB)
[v4] Thu, 1 Feb 2018 08:00:24 UTC (488 KB)
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) Browse v0.1 released 2018-10-22    Feedback?

    About arXiv
    Leadership Team

    Contact Us
    Follow us on Twitter

    Help
    Privacy Policy

    Blog
    Subscribe

arXiv® is a registered trademark of Cornell University.

If you have a disability and are having trouble accessing information on this website or need materials in an alternate format, contact web-accessibility@cornell.edu for assistance.
