
Cornell University
We gratefully acknowledge support from
the Simons Foundation and member institutions.
arXiv.org > math > arXiv:1809.08530
( Help | Advanced search )
Full-text links:
Download:

    PDF
    PostScript
    Other formats

( license )
Current browse context:
math.OC
< prev  |  next >
new  | recent  | 1809
Change to browse by:
cs
cs.LG
math
stat
stat.ML
References & Citations

    NASA ADS

Google Scholar
Bookmark
( what is this? )
CiteULike logo BibSonomy logo Mendeley logo Reddit logo ScienceWISE logo
Mathematics > Optimization and Control
Title: Provably Correct Automatic Subdifferentiation for Qualified Programs
Authors: Sham Kakade , Jason D. Lee
(Submitted on 23 Sep 2018 ( v1 ), last revised 14 Jan 2019 (this version, v2))

    Abstract: The Cheap Gradient Principle (Griewank 2008) --- the computational cost of computing the gradient of a scalar-valued function is nearly the same (often within a factor of 5 ) as that of simply computing the function itself --- is of central importance in optimization; it allows us to quickly obtain (high dimensional) gradients of scalar loss functions which are subsequently used in black box gradient-based optimization procedures. The current state of affairs is markedly different with regards to computing subderivatives: widely used ML libraries, including TensorFlow and PyTorch, do not correctly compute (generalized) subderivatives even on simple examples. This work considers the question: is there a Cheap Subgradient Principle? Our main result shows that, under certain restrictions on our library of nonsmooth functions (standard in nonlinear programming), provably correct generalized subderivatives can be computed at a computational cost that is within a (dimension-free) factor of 6 of the cost of computing the scalar function itself. 

Subjects: 	Optimization and Control (math.OC) ; Machine Learning (cs.LG); Machine Learning (stat.ML)
Cite as: 	arXiv:1809.08530 [math.OC]
  	(or arXiv:1809.08530v2 [math.OC] for this version)
Submission history
From: Jason Lee [ view email ]
[v1] Sun, 23 Sep 2018 04:22:22 UTC (24 KB)
[v2] Mon, 14 Jan 2019 06:09:08 UTC (24 KB)
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) Browse v0.1 released 2018-10-22    Feedback?

    About arXiv
    Leadership Team

    Contact Us
    Follow us on Twitter

    Help
    Privacy Policy

    Blog
    Subscribe

arXivÂ® is a registered trademark of Cornell University.

If you have a disability and are having trouble accessing information on this website or need materials in an alternate format, contact web-accessibility@cornell.edu for assistance.
