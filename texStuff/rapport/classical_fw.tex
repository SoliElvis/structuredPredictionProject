%%% Local Variables:
%%% mode: latex
%%% TeX-master: mainProject.tex
%%% End:
\subsection{Frank\-Wolfe, the conditional gradient algorithm}
Throughout this section we consider the problem of minimizing a
a continusouly differentiable convex function - say $f$ - over some convex set
$\mathcal M$.
\begin{definition}
  A linear minimization oracle over a set $\mathcal M$ is a map defined the following way:
  \begin{equation}
    LMO_{\mathcal{M}}(d) \triangleq \argmin_{s \in\mathcal{M}}\langle d, s \rangle
    \label{eq:lmo}
  \end{equation}
\end{definition}


Intuitively the classical FW is a descent method where starting from some
arbitrary feasible point we repeatdly take convex combinations with outputs of
the oracle $s_{t}= LMO_{\mathcal{M}}(\nabla f(x_{t}))$. 
The convex combination ``weight'', usually denoted $\gamma$,
can be computed exactly analytically. This is a key feature of the algorithm
as many other require to compute the gradient's lipschitz constant and/or the
strong-convexity parameter, usually denoted $\mu$.

\begin{algorithm}[htbp!]
  \caption{Classical Frank-Wolf}
  \label{alg:fw}
\begin{algorithmic}
   \STATE Let $\alpha\in\mathcal{M}$
   \FOR{$k=0$ {\bfseries to} $K$}
   \STATE {Compute $s={\textit{argmin}}_{s\in\mathcal{M}}\langle s, \nabla f(\alpha^{k})\rangle$}
   \STATE Let $\gamma = \frac{2}{k+2}$ (simple version) or optimize for line-search
   \STATE Update $\alpha^{k+1}= (1-\gamma)\alpha^{k}+ \gamma s$
   \ENDFOR
\end{algorithmic}
\end{algorithm}

There has recently been a resugence of interest into FW in the machine learning
literature which can be explained by the following table from a tutorial on FW
by Martin Jaggi \citet{jaggiFrankWolfeOptimizationAlgorithms2014}
\begin{table}[htbp!]
  \center
  \begin{tabular}{|l|l|l|}
  \hline
  & Frank-Wolfe & Gradient Descent and Proximal Methods\\
  \hline
  Iteration cost & solve linear problem & Projection or prox operation more generally\\
  \hline
  Iterates & sparse & \textbf{dense}\\
  \hline
  \end{tabular}
\end{table}

Sparsity is obviously a key feature for preventing the memory required to
explode. Moreover projection and proximal operations can be very expensive in
certain contexts. %TODO

Many variants and extensions of the basic classical FW algorithm
but Jaggi lists the main ones and the active research areas
at the time of writing his paper in 2014. %TODO what changed since then

In this paper we mainly focus on the Block-Seperable variant
applied to struct-svm problems as presented in 
\citet{lacoste-julienBlockCoordinateFrankWolfeOptimization2013}