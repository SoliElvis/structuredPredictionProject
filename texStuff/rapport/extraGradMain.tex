We mainly follow the intuition and proofs of \cite{taskarStructuredPredictionExtragradient}.\\
The dual extragradient algorithm from Nesterov gives a convergence guarantee for
the objective $\mathcal{L}$.\\
We present a simple formulation of the algorithm:
\begin{algorithmic}
  \STATE Initialize: Choose $\hat{\vec u} \in \mathcal{U}$, set $\vec s^{-1} = 0$.
  \FOR{$t=0$ to $t=\tau$}
  \STATE $\vec v = \mathbf{\Pi}_{\mathcal{U}}(\hat{\vec u} + \eta \vec s^{t-1})$\\
  \STATE $\vec u^t = \mathbf{\Pi}_{\mathcal{U}}(\vec v - \eta (\vec F \vec v - \vec a))$\\
  \STATE $\vec s^t =  \vec s^{t-1} - (\vec F \vec u^t - \vec a)$
  \ENDFOR
  \RETURN $\overline{\vec u^{\tau}} = \frac{1}{1 + \tau} \sum_{t=0}^{\tau} \vec u^t$
\end{algorithmic}

This algorithm has a lookahead step (i.e. $v$) that serves the peform the actual
gradient update $u^t$. The intuition behind the lookahead step is that given a
function to optimize that is Lipschitz, Nesterov was able to show that we can
upper bound $f_{D}(\bar{u^n}) = \max_y \left \{ \langle g(y),\bar{u^n} - y
\rangle : d(\hat{u},y) \leq D \right \}$, where $\bar{u^n}$ is the weighted
average over all the updates $u^t$ up to iteration n. The function g corresponds
to the objective $\mathcal{L}$ in our setting. When value of $f_D(\bar{u^n})$
gets close to 0, we have that the value $g(y^*)$ for an optimal $y^*$ is close
to 0, which signifies that we have reached saddle point (i.e. what we wanted).
Nesterov goes on to show that this upper bound indeed goes to 0. We then get
convergence to a saddle point. Note that in the definition of $f_D$, we used a
distance metric d. This corresponds to the Euclidean distance (or Bregman
distance in non-Euclidean setting). The rojection operator $\Pi_{\mathcal{U}}$
in the algorithm simply projects a point back to the set $\mathcal{U}$ by
finding the nearest point with respect to the distance metric used.


\subsubsection{Non-Euclidean setting}
The main problem with the Euclidean projection operator is that for many
problems, it is hard to compute the projection. Indeed for min-cut, we need to
compute the partition function first, which is \#P-complete. Thus, the authors
of the paper introduced the Bregman operator, which computes the projection
using the Bregmand divergence. Using this operator has the great advantage of
being easier to compute. We can see this for $L1$ regularization. Computing a
projection using $L1$ distance is hard since it is not differentiable. Using the
negative entropy, we get that the Bregman divergence is the KL divergence. This
implies that we can differentiate the divergence to get the parameter that
minimizes it.

\subsubsection{Memory-efficient tweak}
In the dual extragradient algorithm, both a vector $s^t$ and a vector
$\bar{u^t}$ are maintained. However, we can observe that the $s_t$'s can be
found using the running average $\bar{u^t}$ since $s^t = -(t + 1 ) \sum_{i=0}^t
(F \bar{u^t} - a)$. We only have to store the vector $\bar{u^t}$. We can even do
better when $|\mathcal{Z}| \gg |\mathcal{W}|$ since $\bar{u^t} = \{
\bar{u^t}_w,\bar{u^t}_z \}$ and we only care about the part that corresponds to
$w$. $\bar{u^t}_z$ is maintained implicitly by storing a vector of size
$|\mathcal{W}|$ (although we now need to store $s_w^t$). It can be reconstructed
using $\bar{u^t}_w$. The following figure (\textbf{ADD FIGURE 5 FROM PAPER})
illustrates the various dependencies.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "mainProject"
%%% End:
