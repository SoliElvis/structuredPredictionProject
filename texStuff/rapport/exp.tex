\subsection{Experiments}
In this section, we describe the implementation we performed for this project.
The goal was to see how the Dual Extragradient algrithm compared to the
Block-coordinate Frank-Wolfe algorithm. The task on which we performed the
evaluation was word alignment in machine translation. We extracted the dataset
<<<<<<< HEAD
from the Europarl dataset \textbf{TODO CITE}. The data consisted of
=======
from the Europarl dataset \cite{. The data consisted of
>>>>>>> b4896b0e1ef113555cb3ff9ea4614c187729ef4b
approximately 2 million sentence pairs in both english and french. Each the
sentences in each pair were translations of one another. We extracted all
sentences and performed a clean by splitting longer sentences into shorter ones.
The goal of this step was to reduce the eventual number of matchings in
training, which could take long to solve using a LP solver. Of course, each
sentence was tokenized beforehand.

We then proceeded to implement the Dual Extragradient and BCFW algorithms using
a SVM. We had to define a feature mapping for an input sentence pair. The features
<<<<<<< HEAD
were extracted using the fastText library \textbf{TODO CITE}. This library included
=======
were extracted using the fastText library \cite{fastText}. This library included
>>>>>>> b4896b0e1ef113555cb3ff9ea4614c187729ef4b
a model that was previously trained to learn embeddings of words in both english
and french. We later combined the embeddings in the two languages by applying a
transformation found in \textbf{TODO CITE}. This transformation consisted in applying
a matrix to each vector in each language (matrix $W$ for english and $Q$ for french). 
These matrices are in fact orthogonal (i.e. $Q^T W = I$). The idea behind such a transformation
is that we sort of ``put" or ``align" both languages in the same vector space, a sort of ``middle ground". 
This way we can better compare the words ``cat" and ``chat" by getting their cosine similarity measure. 
We combined the cosine measure of each pair of words in the alignment by summing. The following blog post
\textbf{CITE THIS} provides a good intuition using maps that are aligned.
As an example, consider the following two sentences:
\begin{itemize}
  \item This assignment was hard
  \item Ce travail etait ardu
\end{itemize}
We would compute the cosine distance for each word tuple (e.g. this/ce, this/travail,
\dots, hard/ardu). We were then able to get the highest match of each english word for
for french translation. This was how we extracted the ``labels". As the dataset was
not annotated with alignments, we had to compute those according to the procedure mentioned.

For the features, we used the concatenated
embeddings of each word pairs in the alignment. This gave us our edge score.
Then, we simply performed a weighted combination of these vectors using the edge
labels as weights. To clarify what we mean by edge labels, suppose that the edge
linking ``assignment'' and ``travail'' has a value of 1. Then, we weight the
vector extracted from these two words with 1. As another example, if the edge
between ``ce'' and ``ardu'' has a value of 0 (i.e. no link between the words),
we do not include the vector computed from the statistics of this word pair.
This is exactly what was done in Taskar \textbf{CITE THIS} modulo some
other features.

In the implementation of BCFW, we used the solver from scipy \textbf{CITE THIS
SHIT} with the simplex method. Since the constraint matrix given by the
optimization of $H_i$ in the algorithm is unimodular, when we relax the LP, we
still get a solution to the ILP without relaxation. Thus, we take advantage of
this fact and indeed use the LP solver. The loss that we used was the $L_1$
distance between the two labels, the proposal and the ground truth.

\subsubsection{Results}
Since running the experiments was computationally intensive and we did not dispose of a
lot of computing power, we had to restrict the training set to a number a relatively small
number of sentence pairs (100). This sanity check was simply a hint to the general applicability 
of the method as we scale up the number of training examples. We used the default $\lambda$ value
of $0.01$ as our regularizer since, we did not want to train for too many iterations before
getting decent results as per Theorem 3 \textbf{CITE THIS}. We were able to get the following
results for the BCFW algorithm:



For the dual extragradient, we were only able to run the algorithm on a dataset
of images \textbf{CITE THE FEC DATASET}. These were tuples of 3 images and a person
hand-picked two images in each 3-tuple that resembled each other the most. Using the
extragradient algorithm, we were able to get convergence but the results were not 
satisfying. We only obtained 46\% accuracy on the prediction, which would compare 
with 17\% if we had a random predictor (3 choose 2 gives 6 possibilites hence 17\%).
Thus, it motivated our change of dataset to obtain better results. We also moved on to
the BCFW algorithm as we had not implemented it yet.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
