\subsection{Extragradient Algorithm for structured predictions}
\subsubsection{Preliminaries}
In the typical setting for SVM's, we aim to optimize a linear objective over a
set of constraints. There are many alernatives to solve that problem, the most
popular ones being gradient-based methods. One of the issues with this type of
algorithm is that they cannot be applied to matchings and min-cuts. In addition,
variants that aim at generating constraints on the fly and incrementally solving
new QP's do not typically scale well. The method proposed in this paper, which
is in essence a modification of the extra-gradient method by Nesterov, leverages
the min-max formulation and takes the dua ; the exponential number of
constraints in then transformed an exponential number of variables. However, the
paper highlights that we can use only a tractable number of these dual variables
to solve the optimization problem. The method prososed in this paper also
generalizes the applicability of the usual extra-gradient method to
non-euclidean, Bergman projections. We thus end up with a framework that is more
flexible and also efficient to implememt.

Since estimating the maximum likelihood over graphical models is often
impractical or infeasible for a wide class of problems, we focus our attention
on large margin estimation. For a dataset $ S = \{ (\vec x_i, \vec y_i)
\}_{i=1}^{m} $, where each $\vec x_i$ is an object with a structure (e.g.
sequence of words in french), we attempt to find the optimal parameter $\vec w$
of a linear classifier:

\begin{equation}
  \vec y_i = \argmax_{\vec y_i' \in \mathcal{Y}} \vec w^T \vec f(\vec x_i,\vec y_i')
  \label{eq1}
\end{equation}

The function $f$ gives a feature mapping of a structured object with its
corresponding label $\vec y_i$. The error of a prediction is mesured by a loss
function $l$. To make the loss convex, another term is introduced in the form a
hinge loss. Since this gives an upper bound to the loss, it is natural to
minimize it. We end up with a problem of the form:

\begin{equation}
  \min_{\vec w \in \mathcal{W}} \sum_i \max_{\vec y_i' \in \mathcal{Y}_i} \left[
\vec w^T \vec f_i(\vec y_i') + l_i(\vec y_i') \right] - \vec w^T \vec f_i(\vec
y_i)
\end{equation}

The parameters $\vec w$ are also regularized with parameter $\lambda$. Since we are
optimizing over $\vec y_i'$, we can drop the term from equation \ref{eq1} and we end
up with a loss-augmented inference problem inside the min function. The three
types of structure that are presented in the paper have a general formulation
that can better be expressed as:

\begin{equation}
  \min_{\vec w \in \mathcal{W}} \max_{\vec z \in \mathcal{Z}} \sum_i \left( \vec
w^T \vec F_i \vec z_i + \vec c_i^T \vec z_i - \vec w^T \vec f_i(\vec y_i)
\right)
  \label{saddle_point}
\end{equation}

where the $\vec z_i$'s can be identified with the edge and node potentials of a
markov network and satisfy the constraints of the structured problem. The terms
$\vec F_i$ correspond to the feature mapping for over all labels $\vec y_i$ when
multiplied by $\vec z_i$'s. The $\vec c_i$'s correspond to the costs of a $\vec z_i$ and can be
identified with the loss $l$ for a label $\vec y_i'$. Taking the dual, we end up with
the following:

\begin{align}
    &\min_{\vec w \in \mathcal{W}, (\vec \lambda,\vec \mu) \geq \vec 0} &\sum_i
\left( \vec b_i^T \lambda_i + \mathbf{1}^T \vec \mu_i - \vec w^T \vec f_i(\vec
y_i) \right)\\ &\text{s.t.} &\vec F_i^T \vec w + \vec c_i \leq \vec A_i^T \vec
\lambda_i + \vec \mu_i \quad i=1,\dots,m
\end{align}

The number of variables and constraints is linear in the number of paramters and
training data. We already see that this formulation is much more efficient. We
do have a set of constraints that is tractable, as is the number of parameters
to update. In equation \ref{saddle_point}, the term that is opitmized is defined
as:

\begin{equation}
  \mathcal{L}(\vec w,\vec z) \triangleq \sum_i \vec w^T \vec F_i \vec z_i + \vec
c_i^T - \vec w^T \vec f_i(\vec y_i)
  \label{saddle_obj}
\end{equation}

It is bilinear in $w$ and $z$. We can then imagine two players represented by
$\vec w$ and $\vec z$ that play a zero-sum game. They perform updates using gradients of
the objective w.r.t. their parameters. They then project the result to the set
of feasible points given by the constraints imposed on the structure. We usually
consider Euclidean projections, as there are well-known problems where they are
efficient to compute. However, as seen later, this is not the case for all
problem. This is why Bregman projections will be introduced. Going back to the
zero-sum game, we have the following operator that is used to perform the
updates for both players at the same time.

\begin{equation*}
 \textit{Let}\quad\vec F= \begin{pmatrix}
 \begin{array}{cccc}
    0 & \vec F_1 & \dots & \vec F_m\\
    -\vec F_1^T & & &\\
    \vdots & & \vec 0 &\\
    -\vec F_m^T & & &
 \end{array}
\end{pmatrix}
\end{equation*}
\begin{equation*}
 \vec u= \begin{pmatrix}
      \begin{array}{c}
        \vec w\\
        \vec z_1\\
        \vdots\\
        \vec z_m
      \end{array}
    \end{pmatrix}\quad\textit{and}\quad
 \vec a=  \begin{pmatrix}
      \begin{array}{c}
        \sum_i \vec f_i(\vec y_i)\\
        \vec c_1\\
        \vdots\\
        \vec c_m
      \end{array}
    \end{pmatrix}
\end{equation*}
\begin{equation*}
 \textit{ such that }\begin{pmatrix}
    \begin{array}{c}
      \nabla_{\vec w} \mathcal{L}(\vec w,\vec z)\\
      -\nabla_{\vec z_1} \mathcal{L}(\vec w,\vec z)\\
      \vdots\\
      -\nabla_{\vec z_m} \mathcal{L}(\vec w,\vec z)
    \end{array}
  \end{pmatrix} = \vec F \vec u - \vec a
\end{equation*}


We can measure the ``goodness'' of the parameters using the gap function
$\mathcal{G}$:
\begin{equation}
  \mathcal{G}(\vec w, \vec z) \triangleq \left[ \max_{\vec z' \in \mathcal{Z}}
\mathcal{L}(\vec w,\vec z') - \mathcal{L}^* \right] + \left[ \mathcal{L}^* -
\min_{\vec w' \in \mathcal{W}} \mathcal{L}(\vec w', \vec z) \right]
\end{equation}

where $\mathcal{L}^*$ gives the result of the min-max of the objective
$\mathcal{L}$. When we have a non-optimal point (i.e. not a saddle point), the
gap is strictly positive. At at an optimal point, the gap is exaclty equal to 0.
Now the restricted gap is exactly the same but the min and max are computed over
a set of parameters that are within a certain distance of the start point
$(\hat{\vec u}_{\vec w},\hat{\vec u}_{\vec z}) \in \mathcal{U}$:
\begin{align}
    \mathcal{G}_{D_{\vec w}, D_{\vec z}}(\vec w, \vec z) = \max_{\vec z' \in
\mathcal{Z}} \left[ \mathcal{L}(\vec w', \vec z') : d(\vec z, \vec z') \leq
D_{\vec z} \right]\\ \quad\quad\quad\quad-\left [ \min_{\vec w' \in \mathcal{W}}
\mathcal{L}(\vec w',\vec z) : d(\vec w, \vec w') \leq D_{\vec w'} \right ]
\end{align}

The motivation for using this restricted gap function is that if we start
``close'' to an optimal point, of course we will converge more rapidly to it.
This can be seen in the convergence analysis of the method.

\subsubsection{Dual Extragradient algorithm}

The dual extragradient algorithm from Nesterov gives a convergence guarantee for
the objective $\mathcal{L}$. The algorithm can be given by:
% \begin{algorithm}[tb]
%    \caption{Dual Extragradient}
%    \label{alg:example}
\begin{algorithmic}
  \STATE Initialize: Choose $\hat{\vec u} \in \mathcal{U}$, set $\vec s^{-1} = 0$.
  \FOR{$t=0$ to $t=\tau$}
  \STATE $\vec v = \mathbf{\Pi}_{\mathcal{U}}(\hat{\vec u} + \eta \vec s^{t-1})$\\
  \STATE $\vec u^t = \mathbf{\Pi}_{\mathcal{U}}(\vec v - \eta (\vec F \vec v - \vec a))$\\
  \STATE $\vec s^t =  \vec s^{t-1} - (\vec F \vec u^t - \vec a)$
  \ENDFOR\\
  \STATE \textbf{return} $\overline{\vec u^{\tau}} = \frac{1}{1 + \tau} \sum_{t=0}^{\tau} \vec u^t$
\end{algorithmic}
%\end{algorithm}

This algorithm has a lookahead step (i.e. $v$) that serves the peform the actual
gradient update $u^t$. The intuition behind the lookahead step is that given a
function to optimize that is Lipschitz, Nesterov was able to show that we can
upper bound $f_{D}(\bar{u^n}) = \max_y \left \{ \langle g(y),\bar{u^n} - y
\rangle : d(\hat{u},y) \leq D \right \}$, where $\bar{u^n}$ is the weighted
average over all the updates $u^t$ up to iteration n. The function g corresponds
to the objective $\mathcal{L}$ in our setting. When value of $f_D(\bar{u^n})$
gets close to 0, we have that the value $g(y^*)$ for an optimal $y^*$ is close
to 0, which signifies that we have reached saddle point (i.e. what we wanted).
Nesterov goes on to show that this upper bound indeed goes to 0. We then get
convergence to a saddle point. Note that in the definition of $f_D$, we used a
distance metric d. This corresponds to the Euclidean distance (or Bregman
distance in non-Euclidean setting). The rojection operator $\Pi_{\mathcal{U}}$
in the algorithm simply projects a point back to the set $\mathcal{U}$ by
finding the nearest point with respect to the distance metric used.
\subsubsection{Proximal step operator}
We define the proximal step operator as follows:
\begin{equation}
  \mathcal{T}_{\eta}(\vec u, \vec s) = \max_{\vec u \in \mathcal{U}} \left \{
\langle \vec s, \vec u' - \vec u \rangle - \frac{1}{\eta} d(\vec u, \vec u')
\leq D \right \}
\end{equation}

The operator is useful to compute projections since when we have a strongly
convex function $h(\vec u)$, we can find its convex conjugate $h^*(\vec u) =
\max_{\vec u \in \mathcal{U}} \left [ \langle \vec s, \vec u \rangle - h(\vec u)
\right ]$. From the definition of a strongly convex function, we have that:
\begin{equation}
  h(\vec u') \geq h(\vec u) + \langle \nabla h(\vec u), \vec u'  - \vec u \rangle + \frac{\sigma}{2} \lVert \vec u' - \vec u \rVert^2
\end{equation}
where $\sigma$ is the strong convexity parameter. Rearranging, we can define an upper bound on the squared norm of $\vec u' - \vec u$. This comes out as:
\begin{equation}
  d(\vec u', \vec u) \triangleq h(\vec u') - h(\vec u) - \langle \nabla h(\vec u), \vec u' - \vec u \rangle \geq \frac{\sigma}{2} \lVert \vec u' - \vec u \rVert^2
\end{equation}

The distance metric $d$ is called the Bregman divergence. The link between the
Bregman diverence and the proximal step operator is that if we are given the
function $h$ inside the definition of the proximal step update, this induces the
Bregman divergence, which in turn induces the update that is performed at each
iteration of the extragradient algorithm. For example, if we have $h(\vec u) =
\frac{1}{2} \lVert \vec u \rVert_2^2 $, the Bregman divergence becomes $d(\vec
u', \vec u) = \frac{1}{2} \lVert \vec u' - \vec u' \rVert_2^2$. We might wonder
why we care about the Bregman divergence when the definition still includes the
usual norm. After all, we still optimize the term $\langle \vec s, \vec u' -
\vec u \rangle - \frac{1}{\eta} d(\vec u', \vec u)$. This is because $h*$ is
differentiable at every point of its domain by the strong convexity of $h$.
Thus, it is easy to compute a projection in the usual fashion: we can compute
the derivative of the termn inside the projection operator and set it to 0. It
is impossible to do for matchings for example as the distance is not even
differentiable. We provide the steps to compute a projection:
\begin{equation}
\begin{aligned}
  &\vec s - \nabla_{\vec u'} d(\vec u', \vec u) = \vec s - \frac{1}{\eta} \nabla_{\vec u'} d(\vec u, \vec u')\\
  &= \vec s - \frac{1}{\eta} \left [\nabla h(\vec u') - \nabla h(\vec u) \right]
\end{aligned}
\end{equation}

By setting this equation to 0, it is possible to recover the optimal $\vec y'$
when, let's say, $h(\vec u) = \frac{1}{2} \lVert \vec u' \rVert^2$.

\subsubsection{Convergence analysis}

The restricted gap function $\mathcal{G}_{D_{\vec v}, D_{\vec z}}$ is upper
bounded by:
\begin{equation}
  \mathcal{G}_{D_{\vec w}, D_{\vec z}}(\overline{\vec w^{\tau}}, \overline{\vec
z^{\tau}}) \leq \frac{\left( D_{\vec w} + D_{\vec z} \right) L}{\tau + 1}
\label{eq:ub}
\end{equation}

In his proof on the convergence of the extragradient algorithm, Nesterov uses a
function $f_D$ instead of $\mathcal{G}_{D_{\vec w}, D_{\vec z}}$, where $f_D$ is
defined as:
\begin{equation}
f_D(\vec x) = \max_{\vec y \in \mathcal{Q}} \left \{ \langle g(\vec y), \vec x -
\vec y \rangle : d(\vec x, \vec y) \right \}
\end{equation}

where the set $\mathcal{Q}$ is the set of parameters and $g$ is a monotone
operator. We can already see a link between the function $f_D$ and the gap
$\mathcal{G}_{D_{\vec w}, D_{\vec z}}$. This comes out as:

\begin{align}
    &\mathcal{G}_{D_{\vec w},D_{\vec z}}(\vec w, \vec z) = \sum_i \vec w^T \vec F_i \vec z_i^* - (\vec w^*)^T \vec F_i \vec z_i - \sum_i (\vec w^T - (\vec w^*)^T ) \vec f_i (\vec y_i)\\
    &\quad\quad\quad\quad- \sum_i \vec c_i^T (\vec z_i - \vec z_i^*)\\
    &= \sum_i (\vec z_i^*)^T \vec F_i^T \vec w - (\vec w^*)^T \vec F_i \vec z_i - \sum_i \vec (\vec f_i (\vec y_i))^T (\vec w - \vec w^*)\\
    &\quad\quad\quad\quad- \sum_i \vec c_i^T (\vec z_i - \vec z_i^*)\\
    &= \sum_i (\vec z_i^*)^T \vec F_i^T (\vec w - \vec w^*) - (\vec w^*)^T \vec F_i (\vec z_i - \vec z_i^*)\\
    &\quad\quad\quad\quad- \sum_i (\vec f_i(\vec y_i))^T (\vec w - \vec w^*) - \sum_i \vec c_i^T (\vec z_i - \vec z_i^*)\\
    &=
    \begin{pmatrix}
      \begin{array}{c}
        \sum_i \vec F_i \vec z_i^*\\
	-\vec F_1^T \vec w^*\\
	\vdots\\
	-\vec F_m^T \vec w^*
      \end{array}
    \end{pmatrix}^T
    \begin{pmatrix}
      \begin{array}{c}
	\vec w - \vec w^*\\
	\vec z_1 - \vec z_1^*\\
	\vdots\\
	\vec z_m - \vec z_m^*
      \end{array}
    \end{pmatrix} -
    \begin{pmatrix}
      \begin{array}{c}
	\sum_i \vec f_i(\vec y_i)\\
	\vec c_1\\
	\vdots\\
	\vec c_m
      \end{array}
    \end{pmatrix}^T
    \begin{pmatrix}
      \begin{array}{c}
	\vec w - \vec w^*\\
	\vec z_1 - \vec z_1^*\\
	\vdots\\
	\vec z_m - \vec z_m^*
      \end{array}
    \end{pmatrix}
\end{align}
From this, we deduce that the function $g$ from the definition of $f_D$ corresponds to:
\begin{equation}
  g(\vec w, \vec z) =
  \begin{pmatrix}
    \begin{array}{c}
      \sum_i \vec F_i \vec z_i^*\\
      - \vec F_i^T \vec w^*\\
      \vdots\\
      - \vec F_m^T \vec w^*
    \end{array}
  \end{pmatrix} -
  \begin{pmatrix}
    \begin{array}{c}
      \sum_i \vec f_i (\vec y_i)\\
      \vec c_1\\
      \vdots\\
      \vec c_m
    \end{array}
  \end{pmatrix} = \vec F \vec u^* - \vec a
\end{equation}
It is constant and thus monotone as required by Nesterov's proof of the
convergence of the algorithm. Its Lipschitz constant $L$ is equal to $\max_{\vec
u \in \mathcal{U}} \lVert \vec F (\vec u - \vec u') \rVert_2 / \lVert \vec u -
\vec u' \rVert_2 \leq \lVert \vec F \rVert_2$. Of course, a point $\vec w, \vec
z$ that satisifies $\lVert \vec w \rVert_2 \leq D_{\vec w}$ and $\lVert \vec z
\rVert_2 \leq D_{\vec z}$ also satisifies $\lVert (\vec w, \vec z) \rVert_2 \leq
D$ when $D = \sqrt{D_{\vec w}^2 + D_{\vec z}^2}$ since $(\vec w, 0) \perp (0,
\vec z)$. It is then easy to see that $f_D \geq \mathcal{G}_{D_{\vec w}, D_{\vec
z}}$. Thus, the function $\mathcal{G}_{D_{\vec w}, D_{\vec z}}$ is upper bounded
by the right-hand side of equation \ref{eq:ub}. We can also observe that the
function $g(\vec w, \vec z)$ is exactly the gradient of the objective
$\mathcal{L}(\vec w, \vec z)$ at the point $\vec w, \vec z$.

\subsubsection{Non-Euclidean setting}
The main problem with the Euclidean projection operator is that for many
problems, it is hard to compute the projection. Indeed for min-cut, we need to
compute the partition function first, which is \#P-complete. Thus, the authors
of the paper introduced the Bregman operator, which computes the projection
using the Bregmand divergence. Using this operator has the great advantage of
being easier to compute. We can see this for $L1$ regularization. Computing a
projection using $L1$ distance is hard since it is not differentiable. Using the
negative entropy, we get that the Bregman divergence is the KL divergence. This
implies that we can differentiate the divergence to get the parameter that
minimizes it.

\subsubsection{Memory-efficient tweak}
In the dual extragradient algorithm, both a vector $s^t$ and a vector
$\bar{u^t}$ are maintained. However, we can observe that the $s_t$'s can be
found using the running average $\bar{u^t}$ since $s^t = -(t + 1 ) \sum_{i=0}^t
(F \bar{u^t} - a)$. We only have to store the vector $\bar{u^t}$. We can even do
better when $|\mathcal{Z}| \gg |\mathcal{W}|$ since $\bar{u^t} = \{
\bar{u^t}_w,\bar{u^t}_z \}$ and we only care about the part that corresponds to
$w$. $\bar{u^t}_z$ is maintained implicitly by storing a vector of size
$|\mathcal{W}|$ (although we now need to store $s_w^t$). It can be reconstructed
using $\bar{u^t}_w$. Figure 1 illustrates the various dependencies.

% \begin{figure}
%   \includegraphics[width=\linewidth]{fig5.png}
%   \caption{Dependency diagram for memory-efficient dual extragradient. The dotted box represents the computations of an iteration of the algorithm.}
%   \label{fig:Dependency diagram}
% \end{figure}


%%%%%%%%%%%%%%%%%       FRANK WOLFE
