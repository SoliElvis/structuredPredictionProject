In this paper we have discussed mainly two methods which were first proposed in
the papers by \citet{taskarStructuredPredictionDual2006} and
\citet{lacoste-julienBlockCoordinateFrankWolfeOptimization2013}. Both of these
papers contain insights that go beyond technicalities but try to cast problems
in some new way and leverage strong guarantees with what seem to be, at the end
of the day, quite strinkingly simple solutions.

The essence of BCFW lies in the simplicity and elegance of convex structures but
also shows how structure can be modular, in the abstraction of the LMO for
computational costs and its equivalence to max oracles for example. In the
ExtraGradient paper a key observation is that the tractability depends on the
very interaction between the task loss, or its scoring scheme, on one hand and
it's structured surrogate loss on the other. As it turns out the combination of
a structured hinge loss as a convex surrogate and a a simple hamming distance
results in a formulation of the \emph{loss-augmented inference} problem for
bipartite matchings as a tractable ILP. In other words if we have a tractable
scoring scheme or task loss, we can map out how the interaction with specific
loss surrogates affects the ``transitivity'' of tractability in the original
problem. These kind of constructive frameworks allow us to build algorithms in a
principled and elegant manner.

The dual extra gradient algorithm presented in
\citet{taskarStructuredPredictionDual2006} casts some of the most common max
margin estimation problems for structured output as a bilinear saddle point
which opened the door for first order already known techniques and using
different generalizations of the concept of projection could provide a new angle
to attack those problems. All those approaches have in common that they leverage
structures through specific black-boxable oracles to assemble algorithms with
strong guarantees or tractability. It is obvious by how he opens his latest
monograph on convex optimization that
\citet{nesterovLecturesConvexOptimization2018} wants to emphasize the need for a
structural thinking pattern when trying to improve tractability of certain
problems.


In terms of structure the saddle-point formulation of
\citet{taskarStructuredPredictionDual2006} to solve structured prediction
problem feels very natural because saddle points are at the very heart of
duality theory which is the main modern tool of convex analysis. As succintly
shown in \cite{botConjugateDualityConvex2010}, the dualization of optimization
problems, in its most general idea, is the creation of a related second problem
to the first by taking the Fenchel conjugate of a given \emph{perturbation
function} which acts a link between the two topological spaces $\mathcal X$ and
$\mathcal Y$.

One should therefore expect their ubiquity and importance in optmization. More
recently \citet{gidelFrankWolfeAlgorithmsSaddle2016a} attacked convex-concave
saddle point problems, i.e. models with the following structure:
\begin{equation}
  \min_{x\in \mathcal X} \max_{y\in\mathcal Y} \mathcal L(x,y)
\end{equation} with a FW angle, i.e. by resctricting themselves to only have
acces to a LMO black box. Instead of alternating projected gradient steps with
some averaging scheme alternating FW steps are performed through convex
combinations once again staying feasible and sparse. Moreover we still get
decent step sizes in the general case, but other situations where we get lucky
as with the dual of struct-svm where the optimal step size ends up being
computable analytically would not be too surpising. In the aforementionned
paper problems considered challenging in optimization such as the
\emph{sparse struct-svm}, i.e. the minimization of the sum of loss-augmented
decoding subproblems with an added $l1$ regularization term. In their
experiments it fares well when compared to state of the art stochastic
subgradient methods. 

All in all the LMO black box approach allow us to build optimization strategies
around pieces of optimization problems which have become solid ``technology'' in
the words of Boyd, i.e. optimization procedures which have been battle tested
and extensively tuned with efficient code. The other key advantage of FW
approaches s the sparsity of iterates can again often be the difference between
impossible and tractable.

Finally seperable approaches are more and more relevant as datasets grow bigger
and we want convergence in less expensive iteration steps. On a higher level,
a fully stochastic treatment of optimization in machine learning contexts, meaning by
treating quantities such as the gradient as random variables to be sampled from,
seems to be a logical step and the seperability of an optimization algorithm
would be hard to neglect as an important factor, even given comparable overall
convergence rates. For instance in \citet{linStochasticPrimalDualProximal2017a}
a stochastic primal dual extragradient method is proposed. Using proximal
mappings instead of projections the author's proposed algorithm, called
Stochastic Primal-Dual Proximal ExtraGradient descent (SPDEG) was shown to
converge in expectation with $\mathcal O (1/\sqrt t)$ rate.

As for the numerical experiments we entertained in order to compare the two
algorithms discussed the results provided with this paper clearly did not
provide reliable benchmarks for other students of the field. However it was a
good reminder that Structured optimization often involves structuring the data
in the first place (in a physical and/or software sence.) Going through the
process of trying to reproduce results underscored the danger of neglecting the
importance of a framework for consistent, solid, and accurate benchmarks. We
found ourselves biasing towards solutions that are easier to preprocess and load
into memory
 -and this might be a harder bias to estimate than the common
statistical one. %%% Local Variables:
%%% mode: latex
%%% TeX-master: "mainProject"
%%% End:
