\subsection{Structured SVM context}
Let us recall the basic goal; to construct an accurate linear classifyer
\begin{equation}
  h_{w}(x)= \arg\max_{y\in \mathcal Y} \, \langle w, \phi(x,y)\rangle
\end{equation}.


We call the $n$-slack formulation from (TODO) of the problem :
\begin{align}
    &\max_{\omega, \xi}\quad\frac{\lambda}{2}||w||^{2}+ \frac{1}{n}\sum_{i=1}^{n}\varepsilon_{i}\\
    &\textit{s.t.}\quad \langle w, \psi_{i}(y)\rangle \geq L(y_{i},y)-
\varepsilon_{i},\quad\forall i ,\forall y\in\mathcal{Y}(x)=\mathcal{Y}_{i}
\end{align}

Consider the \textbf{max oracle}, $\Tilde{H}=
\underset{y\in\mathcal{Y}_{i}}{\textit{max}}$ $\underbrace{L_{i}(y)- \langle w,
\psi_{i}(y)\rangle}_{= H_{i}(y,w)\quad\textit{the hinge loss}}$.\\ (2) The
exponential number of constraints are replaced by $n$ piecewise linear ones. \\

\textbf{Proposition.} The max oracle is a convex upper bound to the task loss.\\
\textit{Proof.} The maximum of two convex (linear) functions is convex, and
\begin{equation*}
\begin{aligned}
    &L(y_{i},h_{w}(x_{i})) \leq L(y_{i},h_{w}(x_{i})) + \underbrace{\langle w, \psi_{i}(y)\rangle}_{\geq 0 \textit{ by definition}} \\
    &\quad\quad\leq \underset{y\in\mathcal{Y}_{i}}{\textit{max}} L_{i}(y)- \langle w, \psi_{i}(y)\rangle
\end{aligned}
\end{equation*}
Thus learning $w$ amounts to the unconstrained problem,
\begin{equation*}
\begin{aligned}
    &\underset{w}{\textit{max}}\quad\frac{\lambda}{2}||w||^{2}+ \frac{1}{n}\sum_{i=1}^{n}\Tilde{H}_{i}(w)
\end{aligned}
\end{equation*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "mainProject"
%%% End:
