\subsection{Structured SVM N-slack formulation}
Let us recall the basic goal; to construct an accurate linear
classifier\footnote{clearly we can easily extend to non-linear cases, with kernel maps for example}
\begin{equation}
  h_{w}(x)= \arg\max_{y\in \mathcal Y} \, \langle w, \phi(x,y)\rangle
\end{equation}

In general finding the optimal seperating hyperplane is an ill-defined problem.
In the support vector machine (SVM) setting we want to find the seperating
hyperplane with the \emph{largest margin}. The support vectors are the
datapoints which lie on the the defined margins. It is not obvious
that we should strive to completely seperate the sets so we can include some
slack variables. Indeed, some set of points might not be linearly separable but
adding slack variables might allow one to misclassify some point but get 
a higher proportion of correctly classified points.\\

We call the following way to pose problem the $n$-slack formulation
\footnote{see \citet{moguerzaSupportVectorMachines2006} for example} of the problem :
\begin{equation}
  \min_{\omega, \xi}\quad\frac{\lambda}{2}||w||^{2}+ \frac{1}{n}\sum_{i=1}^{n}\varepsilon_{i}
  \qquad \textit{s.t.} \quad \langle w, \psi_{i}(y)\rangle \geq L(y_{i},y)-
\varepsilon_{i},\quad\forall i ,\forall y \in\mathcal{Y}(x)=\mathcal{Y}_{i}
\end{equation}

Define $\psi(y) := \phi(\xiii,\yiii) - \phi(\xiii,y)$ and $L_i(y) = L(\yiii, y)$\\

Then the following problem is called the \emph{loss-augmented decoding problem}.
\begin{equation}
H_i = \max_{y\in\mathcal{Y}_{i}} \{L_{i}(y)- \langle w, \psi_{i}(y)\rangle\} 
\end{equation}

We assume it can be solved efficiently (usually through ILPs that have a natural
convex relaxation with integral optimal solutions TODO cite) and black box it
into an oracle which can be sampled from in a transparent way.

Note that the structured hinge-loss is a convex upper bound to the task loss.\\
Hence it is quite natural to see that  learning $w$ amounts to the unconstrained problem,
\begin{equation}
  \min_{w} \quad\frac{\lambda}{2}||w||^{2}+ \frac{1}{n}\sum_{i=1}^{n}H_{i}(w)
  \label{eq:svm-non-smooth}
\end{equation}

This last formulation into a non-smooth unconstrained problem is the one we will
focus throughout this paper. Indeed it allows us to leverage convex analysis
tools which emphasize replacing constraint sets with appropriate additions to
the objective value (which often results in non-smooth problems)
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "mainProject"
%%% End:
