\subsection{Structured SVM context}
Let us recall the basic goal; to construct an accurate linear classifyer
\begin{equation}
  h_{w}(x)= \arg\max_{y\in \mathcal Y} \, \langle w, \phi(x,y)\rangle
\end{equation}.

We call the following way to pose problem the $n$-slack formulation
\footnote{see \citet{moguerzaSupportVectorMachines2006} for example} of the problem :
\begin{align}
    &\max_{\omega, \xi}\quad\frac{\lambda}{2}||w||^{2}+ \frac{1}{n}\sum_{i=1}^{n}\varepsilon_{i}\\
    &\textit{s.t.}\quad \langle w, \psi_{i}(y)\rangle \geq L(y_{i},y)-
\varepsilon_{i},\quad\forall i ,\forall y\in\mathcal{Y}(x)=\mathcal{Y}_{i}
\end{align}

Consider the \textbf{max oracle},
\begin{align}
H_i &= \max_{y\in\mathcal{Y}_{i}} \quad \{L_{i}(y)- \langle w, \psi_{i}(y)\rangle\} \\
\mathcal H_i &= \argmax_{y\in\mathcal{Y}_{i}} \quad \{ L_{i}(y)- \langle w, \psi_{i}(y)\rangle \}
\end{align}

\textbf{Proposition.} The max oracle is a convex upper bound to the task loss.\\
\textit{Proof.} The maximum of two convex (linear) functions is convex, and
\begin{equation*}
\begin{aligned}
    &L(y_{i},h_{w}(x_{i})) \leq L(y_{i},h_{w}(x_{i})) + \underbrace{\langle w, \psi_{i}(y)\rangle}_{\geq 0 \textit{ by definition}} \\
    &\quad\quad\leq \underset{y\in\mathcal{Y}_{i}}{\textit{max}} L_{i}(y)- \langle w, \psi_{i}(y)\rangle
\end{aligned}
\end{equation*}
Thus learning $w$ amounts to the unconstrained problem,
\begin{equation*}
\begin{aligned}
    &\underset{w}{\textit{max}}\quad\frac{\lambda}{2}||w||^{2}+ \frac{1}{n}\sum_{i=1}^{n}\Tilde{H}_{i}(w)
\end{aligned}
\end{equation*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "mainProject"
%%% End:
