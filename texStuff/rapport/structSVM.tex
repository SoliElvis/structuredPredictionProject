\subsection{Structured SVM context}
Given a training set $\mathcal{D}=\{(x_i,y_i)\}_{i=1}^n$ where $y \in
\mathcal{Y}$ is a multi-label output, and a feature map
$\phi:\mathcal{X}\times\mathcal{Y}\longrightarrow \mathbb{R}$, which encodes a
similarity measure between $\mathcal{X}$ and $\mathcal{Y}$, such that if $y_{i}$
is the ground truth (target) for an input $x_{i}$, then
\begin{equation*}
\begin{aligned}
    &\forall y\in\mathcal{Y}\texttt{\symbol{92}}\{y_{i}\}\quad\textit{we
have}\quad \psi_{i}(y) = \phi(x_{i},y_{i})- \phi(x_{i},y) > 0
\end{aligned}
\end{equation*}
The aim is to construct an accurate linear classifyer, $h_{w}(x)=
\underset{y\in\mathcal{Y}(x)}{\textit{argmax}}\langle w, \phi(x,y)\rangle$.\\ To
learn $w$, consider the task loss
$L:\mathcal{Y}\times\mathcal{Y}\longrightarrow\mathbb{R}_{+}$, where
$L(y,y\prime)= 0 \Longleftrightarrow y= y\prime$. \\

The $n$-slack formulation of the problem would be,
\begin{equation*}
\begin{aligned}
    &\underset{w,\xi}{\textit{max}}\quad\frac{\lambda}{2}||w||^{2}+ \frac{1}{n}\sum_{i=1}^{n}\varepsilon_{i}\\
    &\textit{s.t.}\quad \langle w, \psi_{i}(y)\rangle \geq L(y_{i},y)-
\varepsilon_{i},\quad\forall i ,\forall y\in\mathcal{Y}(x)=\mathcal{Y}_{i}
\end{aligned}
\end{equation*}
\textit{Problems:} (1) The zero-one loss is not differentiable and (2) we have
an exponential number of constraints.\\ \textit{Solutions:} (1) Minimizing an
upper bound to the task loss gives us a worst case guarantee.\\


Consider the \textbf{max oracle}, $\Tilde{H}=
\underset{y\in\mathcal{Y}_{i}}{\textit{max}}$ $\underbrace{L_{i}(y)- \langle w,
\psi_{i}(y)\rangle}_{= H_{i}(y,w)\quad\textit{the hinge loss}}$.\\ (2) The
exponential number of constraints are replaced by $n$ piecewise linear ones. \\

\textbf{Proposition.} The max oracle is a convex upper bound to the task loss.\\
\textit{Proof.} The maximum of two convex (linear) functions is convex, and
\begin{equation*}
\begin{aligned}
    &L(y_{i},h_{w}(x_{i})) \leq L(y_{i},h_{w}(x_{i})) + \underbrace{\langle w, \psi_{i}(y)\rangle}_{\geq 0 \textit{ by definition}} \\
    &\quad\quad\leq \underset{y\in\mathcal{Y}_{i}}{\textit{max}} L_{i}(y)- \langle w, \psi_{i}(y)\rangle
\end{aligned}
\end{equation*}
Thus learning $w$ amounts to the unconstrained problem,
\begin{equation*}
\begin{aligned}
    &\underset{w}{\textit{max}}\quad\frac{\lambda}{2}||w||^{2}+ \frac{1}{n}\sum_{i=1}^{n}\Tilde{H}_{i}(w)
\end{aligned}
\end{equation*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "mainProject"
%%% End:
