\subsection{Block Coordinate Frank Wolfe}
Due to the exponential number of dual variables in the structured SVM setting,
classical algorithms like projected gradient are intractable. Stochastic
subgradient methods, on the other hand, achieve a sublinear convergence rate
while only requiring a single call to the maximization oracle every step. They
are nonetheless very sensitive to the sequence of stepsizes and it is unclear
when to terminate the iterations. 

In fact many algorithms with good theoretical garantees introduce a dependency
between the stepsize to be chosen and some constants which caracterize the
function, e.g. the lipschitz constant of the gradient ($L$) and/or the
strong-convexity constant ($\mu$) in order to provide some convergence
garantees. For example Nesterov (2003) in his analysis of dual extragradient the
step size is bounded above by the gradient's Lipschitz constant which happens to
correspond to the norm of the $F$ matrix as defined in the previous sextion.
Projections and proximal operators can involve quadratic programs but the
natural convex combination approach ensures feasibility at no cost. Since the
dual objective function for structured-svm is a quadratic problem we can compute
analytically exactly the optimal line-search step size. The BCFW algorithm
presented in \citet{lacoste-julienBlockCoordinateFrankWolfeOptimization2013}
also shows a couple equivalencies, the most important being that solving the
loss-augmented problem is equivalent to a call to the linear minimization oracle
in the dual.

Essentially BCFW can be seen as generalizing some subgradient and cutting plane
methods. Tractability in the end, in the current literature, depends on how the
inference problem (usually cast as an ILP since we are mostly dealing we
underlying structures of combinatorial nature) and the task loss interact. It is
shown in \cite{taskarStructuredPredictionDual2006} that in many interesting
cases such as when the weighted hamming distance is used as task loss for
bipartite matching; the inference augmented loss inherits tractability and we
have an LP with optimal integral solutions. It is a well known fact in
combinatorial optimization that the incidence matrix of a bipartite graph is
awlays totally unimodular which is a sufficient condition for optimal integral
solution in its convex relaxation LP. The loss-augmented problems defined by the
interaction of task loss and its convexification through the hinge loss
``operator'' are thus related in a very natural way to diverse combinatorial
problems and the BCFW provides a theoretical pivot between those two problems.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "mainProject"
%%% End:
