\subsection{Frank-Wolfe, the conditional gradient algorithm}
\subsubsection*{Classical Frank-Wolfe}
Consider a linear minimization oracle,
\begin{equation}
    LMO_{\mathcal{A}}(\nabla f(x_{t}))\in
    \argmin_{s\prime\in\mathcal{A}}\langle s\prime, \nabla f(x_{t})\rangle
    \label{eq:lmo}
\end{equation}
Simply put; starting with an active set consisting of only an initial feasible
point $S^{0}= \{x_{0}\}$, the Frank-Wolfe algorithm adds an element $s_{t}=
LMO_{\mathcal{A}}(\nabla f(x_{t}))$, to the active set by taking its
convex combination with the previous iterate which allows for a sparse
representation. Sparsity is a key feature of the BCFW variant of Frank Wolfe as
it optimizes over the dual of struct-SVM which has a very large number of
variables (since the primal has a very large number of constraints defined
through the hinge loss)
\begin{algorithm}
  \caption{Classical Frank-Wolf}
  \label{alg:fw}
\begin{algorithmic}
   \STATE Let $\alpha\in\mathcal{M}$
   \FOR{$k=0$ {\bfseries to} $K$}
   \STATE {Compute $s={\textit{argmin}}_{s\in\mathcal{M}}\langle s, \nabla f(\alpha^{k})\rangle$}
   \STATE Let $\gamma = \frac{2}{k+2}$, or optimize $\gamma$ by line search
   \STATE Update $\alpha^{k+1}= (1-\gamma)\alpha^{k}+ \gamma s$
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection*{Convergence results}
We define the duality gap
\begin{equation}
    g(\alpha^{k})= \underset{s\in\mathcal{M}}{\textit{max}}\langle \alpha^{k}-s, \nabla f(\alpha^{k})\rangle
\end{equation}
By first order convexity of the objective, we have
\begin{align}
    &f(s)\geq f(\alpha^{k})+ \langle \alpha^{k}-s, \nabla f(\alpha^{k})\rangle\\
    &\Longrightarrow g(\alpha^{k})=
      \min_{s\in\mathcal{M}}\langle \alpha^{k}-s, \nabla
      f(\alpha^{k})\rangle \geq f(\alpha^{k})- f^{*}
\end{align}
We can thus see that the duality gap gives us a computable optimality guarantee. \\

\begin{definition}
  The curvature constant $C_{f}$ is given by the maximum relative deviation of
  the objective function f from its linear aaporximations, over the domain
  $\mathcal{M}$,
\end{definition}


\begin{align}
    &C_{f}= \underset{\underset{ \gamma\in[0,1],
y=x+\gamma(s-x)}{x,s\in\mathcal{M}}}{sup}\frac{2}{\gamma^{2}}\Big(f(y)- f(x)-
\langle y-x, \nabla f(x)\rangle\Big)
\end{align}
Intuitively, the curvature constant can be seen as a measure of how flat the
objective function is. For example, if the objective is linear, say $f(x)= ax+
b$ and $x\in[e,f]$ then $\nabla f(x)= a$ and the curvature constant is zero:
\begin{align}
    &C_{f}= \frac{2}{\gamma^{2}}\Big(ay+ b- ax- b +(-ay +ax)\Big)= 0
\end{align}
Moreover $s=\textit{argmin}_{s\in[e,f]}\langle s, a\rangle= \frac{e}{a}$. Hence
we reach the minimum in one F-W step. Thus, we can observe that for flatter
functions, that is with smaller curvature constants, Frank-Wolfe should converge
faster. 

\begin{theorem}
  The duality gap obtained in the $t^{th}$ iteraton of the Frank-Wolfe algorithm
satisfies
\begin{align}
    &g(x_{t})\leq 2\beta\frac{C_{f}}{t+2}(1+\delta)
\end{align}
Where $\beta= \frac{27}{8}$ and $\delta$ is the approximation error tolerated in
the $LMO$.
\end{theorem}

\subsection{Block Coordinate Frank Wolfe}
Due to the exponential number of dual variables in the structured SVM setting,
classical algorithms, like projected gradient are intractable. Stochastic
subgradient methods, on the other hand, achieve a sublinear convergence rate
while only requiring a single call to the maximization oracle every step. They
are nonetheless very sensitive to the sequence of stepsizes and it is unclear
when to terminate the iterations. \\

Frank-Wolfe methods address these problems by giving a stepsize which
can be computed exactly analytically and a computable duality gap while still
retaining a sublinear convergence rate. Moreover, despite the exponential number
of constraints, the algorithm has sparse iterates alleviating the memory issues
which come with the exponential number of dual variables.

\begin{theorem} Given a convex, differentiable objective
$f:\mathcal{M}^{1}\times...\times\mathcal{M}^{n}\to\mathbb{R}$, where $\forall
i\in\{1..n\}$, each factor\quad $\mathcal{M}^{i}\subseteq\mathbb{R}^{n}$ is
convex and compact, if we are at a point $x$ such that $f(x)$ is minimized along
each coordinate axis, then $x$ is a global minimum.
\begin{proof}
The objective function being differentiable and convex, if we are at a point
$\alpha$ such that $f(\alpha)$ is minimized along each coordinate axis, then
$\alpha$ is a global minimizer. Therefore,
\begin{align}
    &\underset{s\in\mathcal{M}}{\textit{min}}\langle s, \nabla f(\alpha)\rangle
= \sum_{i}\underset{s_{i}\in\Delta_{|\mathcal{Y}_{i}|}}{\textit{min}}\langle
s_{i}, \nabla_{i} f(\alpha)\rangle
\end{align}

Moreover, with
\begin{align*}
   &w=A\alpha, A=\Big[\frac{1}{n\lambda}\psi_{1}(y)...\frac{1}{n\lambda}\psi_{\sum_{i}|\mathcal{Y}_{i}|}(y)\Big]\\
   &\textit{and}\quad b=\Big(\frac{1}{n}L_{i}(y)\Big)_{i\in\big[n\big],y\in\mathcal{Y}_{i}}
\end{align*}
The gradient of the dual would be,
\begin{align*}
    &\nabla f(\alpha)= \nabla\Big[\frac{\lambda}{2}||A\alpha||^{2}- b^{T}\alpha\Big] = \lambda A^{T}A\alpha- b\\
    &= \lambda A^{T}w- b= \frac{1}{n}H_{i}(y,w)\\
    &\underset{y_{i}\in\mathcal{Y}_{i}}{\textit{max}}\quad\tilde{H}_{i}=
  -\underset{y_{i}\in\mathcal{Y}_{i}}{\textit{min}}\quad\tilde{H}_{i} =
  \underset{y_{i}\in\mathcal{Y}_{i}}{\textit{min}}\quad L_{i}- \langle w,
  \psi_{i}\rangle\\ &=
  \underset{s_{i}\in\Delta_{|\mathcal{Y}_{i}|}}{\textit{min}}\langle s_{i},
  \nabla_{i} f(\alpha)\rangle\\
\end{align*}
Thus we can see that, if $n=$ size of the training data, one Frank-Wolfe step is
equivalent to $n$ calls to the maximization oracle.
\end{proof}
\end{theorem}

\begin{algorithm}
  \caption{Block Seperable Frank-Wolf}
  \label{alg:bcgfw}
\begin{algorithmic}
   \STATE Let $\alpha\in\mathcal{M}$
    \STATE {Let $w^{0}= 0$, $l^{0}= 0$}\\
    \FOR{$k=0, \dots, K$}
      \FOR{$i=1, \dots, n$}
        \STATE {Solve $y_{i}^{*}=\underset{y_{i}\in\mathcal{Y}_{i}}{\max} H_{i}(y,w^{k})$}\\
        \STATE {Let $w_{s}= \sum_{i=1}^{n}\frac{1}{n\lambda}\psi_{i}(y_{i}^{*})$,
                and $l_{s}= \frac{1}{n}\sum_{i=1}^{n}L_{i}(y_{i}^{*})$}\\
        \STATE {Let $\gamma= \frac{\lambda(w^{k}-w_{s})^{T}w^{k}- l^{k}+ l_{s}}{\lambda||w^{k}-w_{s}||^{2}}$,
                and clip to $[0,1]$}\\
        \STATE {Update $w^{k+1}= (1-\gamma)w^{k}+ \gamma w_{s}$,
                and $l^{k+1}= (1-\gamma)l^{k}+ \gamma l_{s}$}\\
      \ENDFOR
   \ENDFOR
\end{algorithmic}
\end{algorithm}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "mainProject"
%%% End:
