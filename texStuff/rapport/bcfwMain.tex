\subsection{Block Coordinate Frank Wolfe}
Due to the exponential number of dual variables in the structured SVM setting,
classical algorithms like projected gradient are intractable. Stochastic
subgradient methods, on the other hand, achieve a sublinear convergence rate
while only requiring a single call to the maximization oracle every step. They
are nonetheless very sensitive to the sequence of stepsizes and it is unclear
when to terminate the iterations. \\

In fact many algorithms with good theoretical garantees introduce a dependency
between the stepsize to be chosen and some constants which caracterize the
function, e.g. the lipschitz constant of the gradient ($L$) and/or the
strong-convexity constant ($\mu$). For example Nesterov (2003) in his analysis
of dual extragradient the step size is bounded above by the gradient's Lipschitz
constant which happens to correspond to the norm of the $F$ matrix as defined in
the previous sextion.\\

Those projection-like operators are, however ingenious in their design, not
necessarily computable. One of the main interests in FW approaches is that we
get an optimality certficate \textit{for free} at each iteration. The estimate
used ties in elegantly with convex analysis results. Moreover we have many good
options for the choice of step-size with theoretical guarantees.\\

Leveraging those aspects the Block-Separable Frank Wolfe algorithm proposed in
\cite{lacoste-julienBlockCoordinateFrankWolfeOptimization2013} can be used with
a very simple analytic for the step-size while ensuring theoretical convergence.
That being said there are many variants which have been tried which use line
search like methods. We also get, \textit{for free} an easily computable duality
gap estimate while still retaining a sublinear convergence rate. Moreover,
despite the exponential number of constraints, the algorithm has sparse iterates
alleviating the memory issues which come with the exponential number of dual
variables. 

 \begin{algorithm}[htbp!]
    \caption{Block-Coordinate Frank-Wolfe -- 
\citet{lacoste-julienBlockCoordinateFrankWolfeOptimization2013}}
    \label{alg:bcfw}
\begin{algorithmic}
\STATE {Let $w^{0}= w_{i}^{0}= \overline{w}^{0}= 0$, $l^{0}= l_{i}^{0}= 0$}\\
  \FOR {$k=0...K$}{
      \STATE{Pick $i$ at random in $\{1,...,n\}$}\\[1ex]
      \STATE {Solve $y_{i}^{*}=\underset{y_{i}\in\mathcal{Y}_{i}}{\textit{max}}\quad H_{i}(y,w^{k})$}\\[1ex]
      \STATE {Let $w_{s}= \frac{1}{n\lambda}\psi_{i}(y_{i}^{*})$, and $l_{s}= \frac{1}{n}L_{i}(y_{i}^{*})$}\\[1ex]
      \STATE {Let $\gamma= \frac{\lambda(w_{i}^{k}-w_{s})^{T}w^{k}- l_{i}^{k}+
              l_{s}}{\lambda||w_{i}^{k}-w_{s}||^{2}}$, and clip to $[0,1]$}\\[1ex]
      \STATE {Update $w_{i}^{k+1}= (1-\gamma)w_{i}^{k}+ \gamma w_{s}$, and
              $l_{i}^{k+1}= (1-\gamma)l_{i}^{k}+ \gamma l_{s}$}\\[1ex]
      \STATE {Update $w^{k+1}= w^{k}+ w_{i}^{k+1}- w_{i}^{k}$, and
              $l_{i}^{k+1}= (1-\gamma)l_{i}^{k}+ \gamma l_{s}$}\\[1ex] }
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\clearpage

The version of BCFW presented above \ref{alg:bcfw} is the result of applying
FW to the (Lagrange) dual of the struct-svm. Let $f$ be the objective function
of the dual problem of svm-truct. Then, as sensibly emphasized in the paper,
the key insight is to notice that pluggin $\nabla f(\hat x)$ in the linear
minimization oracle $LMO_{\mathcal M_i}$ is equivalent to solving the
loss-augmented decoding problem. \\

This equivalency means we can leverage the clean theoretical results from
classical FW provided the usually ``moderate'' requirements are satisfied. Hence
this algorithm is a theoretically sound solution to the main impediments of
comparable cutting plane and subgradient methods with respect to their
application in modern machine learning applications: cutting plane is no a
separableh method and subgradient methods have some step-size issues as
mentionned previously.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "mainProject"
%%% End:
